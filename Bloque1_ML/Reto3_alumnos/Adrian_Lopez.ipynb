{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 3: El problema final\n",
    "\n",
    "Este reto consiste en predecir si una imagen contiene o no signos de retinopatía diabética (DR).\n",
    "Para ello se proporciona un conjunto de datos que contiene características extraídas de imágenes.\n",
    "Las características extraídas son:\n",
    "+ [0] Evaluación de la calidad de la imagen , donde 0 = mala calidad 1 = calidad suficiente.\n",
    "+ [1] El resultado binario de la evaluación previa, donde 1 indica una anormalidad retiniana severa y 0 su falta.\n",
    "+ [2-7] Los resultados de la detección de microaneurismas (MA). Cada valor de característica representa el número de MAs encontrados en los niveles de confianza alfa = 0.5, ... , 1, respectivamente.\n",
    "+ [8-15] contienen la misma información que [2-7] para los exudados. <br>\n",
    "Sin embargo, ya que los exudados están representados por un conjunto de puntos en lugar del número de\n",
    "píxeles de las lesiones, estas características se normalizan dividiendo los número de lesiones por  el diámetro de la ROI para compensar las diferentes tamaños de las imágenes.\n",
    "+ [16] La distancia euclidiana del centro dela mácula y el centro del disco óptico para proporcionar información importante con respecto a la condición del paciente. Esta característica también se normaliza con el diámetro de la ROI\n",
    "+ [17] El diámetro del disco óptico.\n",
    "+ [18] El resultado binario de la clasificación basada en AM/FM.\n",
    "+ [19] Etiqueta de clase. 1 = contiene signos de DR\n",
    "\n",
    "El conjunto de datos, *retinopatia_reto3.csv*, consiste en 592 muestras de imágenes; cada uno de ellos representado por 19 características.\n",
    "\n",
    "Para evaluar las propuestas se utilizará un conjunto de datos que se mantendrá oculto hasta después de la entrega\n",
    "\n",
    "### Requisitos\n",
    "+ **Se debe utilizar algún tipo de reducción de la dimensión o aprendizaje de variedades**\n",
    "+ A continuación se debe entrenar un clasificador o combinación de clasificadores con las características transformadas según el requisito anterior.\n",
    "+ Se debe entregar un cuaderno Jupyter con el nombre de los participantes.<br>\n",
    "  *Por ejemplo*:   **Cuesta_Hinton.ipynb**\n",
    "+ El cuaderno entregado debe seguir la estructura y reglas de este cuaderno\n",
    "\n",
    "### Competición\n",
    "+ Todos los cuadernos entregados se subirán al repo de GitHub y se ejecutarán en Binder, donde ya estará en conjunto de test que estaba oculto.\n",
    "+ El resultado que se obtenga será la puntuación del reto.\n",
    "+ **Importante** Es muy fácil asegurarte de que tu código funciona:\n",
    "    1. Agrupa todo tu código en una única celda\n",
    "    2. Copialo en el cuaderno del reto que hay en Binder\n",
    "    3. Ejecuta el cuaderno \n",
    "    \n",
    "### Plazo: lunes 16 de nov. a las 6 am.\n",
    "Es decir, incluye toda la noche del domingo 15 de nov.\n",
    " \n",
    "> \"The final problem\", A. C. Doyle (Strand Magazine, diciembre 1983), \n",
    "es el relato corto en el que Sherlock Holmes se enfrenta al Prof. Moriarty en las cataratas Reichenbach.\n",
    "\n",
    "---\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR ESTA CELDA\n",
    "# Conjunto distribuido para el reto\n",
    "\n",
    "Challange_filename = '../../Datasets/retinopatia_reto3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR ESTA CELDA\n",
    "# El conjunto de test cambiará una vez se cierre la entrega\n",
    "# Ahora mismo es el mismo conjunto que el de entrenamiento\n",
    "\n",
    "Test_filename = '../../Datasets/retinopatia_test.csv' #<-- este nombre cambiará después del plazo de entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR ESTA CELDA\n",
    "\n",
    "#-[1]. Load data from CSV and put all in a single dataframe 'FullSet'\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../../MyUtils/')\n",
    "import MyUtils as my\n",
    "seed = 1234 #<- random generator seed (comment to get randomness)\n",
    "\n",
    "#-[2]. Load data from CSV and put all in a single dataframe 'FullSet'\n",
    "\n",
    "FullSet = pd.read_csv(Challange_filename, header=0)\n",
    "FullX = FullSet.iloc[:,:-1]\n",
    "FullY = FullSet.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"> \n",
    " <tr style='background:lime'>\n",
    "  <td style=\"text-align:left\">\n",
    "      <h2>Tu código debe empezar a partir de aquí y puede tener tantas celdas como quieras</h2>\n",
    "      <p> Si quieres, puedes borrar (o convertir en RawNBConvert) las celdas de ejemplo\n",
    "      <h3>Importante:</h3>\n",
    "      <p>Tu código debe producir las siguientes variables: </p>\n",
    "      <p> $\\quad \\bullet$ <b>clf:</b> el clasificador final con el que se realizará el test<br>\n",
    "       $\\quad \\bullet$ <b>X_test:</b> el conjunto de test listo para ser usado por el método <b>predict</b><br>\n",
    "       $\\quad \\bullet$ <b>Y_test:</b> es el vector de etiquetas del conjunto de X_test listo para ser usado por el método <b>confusion_matrix</b>\n",
    "      </p>\n",
    "  </td>\n",
    " </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mi solución consiste en un clasificador por votación \"soft\" que está formada por los siguientes clasificadores:\n",
    "\n",
    "    - Random Forest con 150 árboles de 2 de profundidad\n",
    "    - Un SVC con los mejores hiperparámetros encontrados\n",
    "    - Un AdaBoost usando Naive Bayes\n",
    "    - Un bagging utilizando 1000 combinaciones de la máquina SVC con los méjores hiperparámetros encontrados\n",
    "    \n",
    "La reducción de dimensionalidad se ha realizado con un LLA (variedades) de 10 componentes y usando 50 vecinos para el cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = [\"Adrian Lopez\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "test_size = 0.25\n",
    "n_components = 10\n",
    "n_neighbors  = 50\n",
    "method = \"standard\"   \n",
    "neighbors_algorithm = \"auto\" \n",
    "\n",
    "lle= LocallyLinearEmbedding(n_components = n_components, n_neighbors=n_neighbors,\n",
    "                             method = method, neighbors_algorithm = neighbors_algorithm)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# X_train, Y_train, X_test, Y_test = \\\n",
    "#    my.single_stratified_split(FullX,FullY, test_size=test_size, random_state=seed)\n",
    "\n",
    "X_train = FullX\n",
    "Y_train = FullY\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = lle.fit_transform(X_train)\n",
    "Y_train = Y_train.values.ravel() \n",
    "\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_test = lle.transform(X_test)\n",
    "# Y_test = Y_test.values.ravel() \n",
    "\n",
    "FullSet = pd.read_csv(Test_filename, header=0)\n",
    "TestX_ = FullSet.iloc[:,:-1]\n",
    "TestY_ = FullSet.iloc[:,-1]\n",
    "X_test = scaler.transform(TestX_) \n",
    "X_test = lle.transform(X_test) \n",
    "Y_test = TestY_.values.ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = 150\n",
    "max_leaf_nodes = 500\n",
    "max_depth = 2\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                max_depth=max_depth)\n",
    "\n",
    "RF = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=4.39174256027293, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=5718.963290975593,\n",
       "    kernel='rbf', max_iter=-1, probability=True, random_state=1234,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM = SVC(kernel='rbf', C=4.39174256027293, gamma=5718.963290975593, random_state = seed,probability=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# BUSQUEDA DE MEJORES HIPERPARÁMETROS PARA UNA SVM\n",
    "\n",
    "# C =  1.134909733040956  Gamma =  4.699164539492036\n",
    "# C =  4.39174256027293  Gamma =  5718.963290975593\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = []\n",
    "clfs = []\n",
    "\n",
    "C = []\n",
    "gamma = []\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    C = np.append(C,aux)\n",
    "    aux = aux*1.07\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    gamma = np.append(gamma,aux)\n",
    "    aux = aux*1.07\n",
    "\n",
    "contador = 0\n",
    "for i in C:\n",
    "    contador+=1\n",
    "    for j in gamma:\n",
    "        clf = SVC(kernel='rbf', C=i, gamma=j, random_state = seed, decision_function_shape = 'ovo')\n",
    "        clf.fit(X_train,Y_train)\n",
    "        score = clf.score(X_test,Y_test)\n",
    "        scores = np.append(scores,score)\n",
    "        clfs = np.append(clfs,clf)\n",
    "        print(np.round(contador/3,2),\"%      Last Score: \",score, \"     Max_Score: \", np.max(scores))\n",
    "        if np.max(scores) == score:\n",
    "            print(\"C = \", i,\" Gamma = \", j)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# METODO RECHAZADO\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_estimators = 150\n",
    "learning_rate= .01\n",
    "\n",
    "lin_clf = SVC(kernel='linear',C=1,probability=True)\n",
    "clf = AdaBoostClassifier(lin_clf, n_estimators=n_estimators, algorithm=\"SAMME.R\", learning_rate=learning_rate)\n",
    "lin_SVM = clf\n",
    "# clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "n_estimators = 25\n",
    "learning_rate= .001\n",
    "\n",
    "nbc_clf  = GaussianNB()\n",
    "clf = AdaBoostClassifier(nbc_clf, n_estimators=n_estimators, algorithm=\"SAMME.R\", learning_rate=learning_rate)\n",
    "NBC = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LEARN Bagging ensemble --------------\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_estimators= 1000\n",
    "max_samples = 350\n",
    "bootstrap = True\n",
    "\n",
    "svm_clf = SVC(kernel='rbf',  C=4.39174256027293, gamma=5718.963290975593, random_state = seed)\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=svm_clf,\n",
    "                            bootstrap=bootstrap, n_estimators=n_estimators, max_samples=max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('RF',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=2,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=150,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0...\n",
       "                                                                   gamma=5718.963290975593,\n",
       "                                                                   kernel='rbf',\n",
       "                                                                   max_iter=-1,\n",
       "                                                                   probability=False,\n",
       "                                                                   random_state=1234,\n",
       "                                                                   shrinking=True,\n",
       "                                                                   tol=0.001,\n",
       "                                                                   verbose=False),\n",
       "                                                bootstrap=True,\n",
       "                                                bootstrap_features=False,\n",
       "                                                max_features=1.0,\n",
       "                                                max_samples=350,\n",
       "                                                n_estimators=1000, n_jobs=None,\n",
       "                                                oob_score=False,\n",
       "                                                random_state=None, verbose=0,\n",
       "                                                warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_system = 'soft'\n",
    "\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('RF', RF), ('NBC', NBC), ('SVM',SVM),(\"BAG\",bag)],\n",
    "    voting=voting_system)\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GMM: METODO RECHAZADO\n",
    "\n",
    "# PREPARACIÓN DE DATOS\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "seed = 1234\n",
    "n_components = 5\n",
    "n_neighbors  = 30\n",
    "method = \"standard\"   # options are: \"standard\", \"hessian\", \"modified\" or \"ltsa\"\n",
    "neighbors_algorithm = \"auto\" # options are : \"auto\", \"brute\", \"kd_tree\", \"ball_tree\"\n",
    "\n",
    "lle= LocallyLinearEmbedding(n_components = n_components, n_neighbors=n_neighbors,\n",
    "                             method = method, neighbors_algorithm = neighbors_algorithm)\n",
    "\n",
    "test_size = 0.25\n",
    "\n",
    "\n",
    "# Dividimos el DataSet por etiquetas y escalamos las características de cada una por separado\n",
    "FullX = FullSet.iloc[:,:-1]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(FullX)\n",
    "X_proy = lle.fit_transform(X_scaled)\n",
    "\n",
    "Full_ready = pd.concat([pd.DataFrame(X_proy),FullY],axis=1)\n",
    "\n",
    "Full0 = Full_ready.drop(FullSet[FullSet['19']!=0].index).reset_index(drop=True)\n",
    "Full1 = Full_ready.drop(FullSet[FullSet['19']!=1].index).reset_index(drop=True)\n",
    "\n",
    "FullX0 = Full0.iloc[:,:-1]\n",
    "FullY0 = Full0.iloc[:,-1]\n",
    "\n",
    "FullX1 = Full1.iloc[:,:-1]\n",
    "FullY1 = Full1.iloc[:,-1]\n",
    "\n",
    "X0_train, Y0_train, X0_test, Y0_test = \\\n",
    "   my.single_stratified_split(FullX0,FullY0, test_size=test_size, random_state=seed)\n",
    "X1_train, Y1_train, X1_test, Y1_test = \\\n",
    "   my.single_stratified_split(FullX1,FullY1, test_size=test_size, random_state=seed)\n",
    "\n",
    "X_test = np.append(X0_test,X1_test,axis=0)\n",
    "Y_test = np.append(Y0_test,Y1_test,axis=0)\n",
    "\n",
    "# CREAMOS LOS GMM (1 PARA CADA CLASE)\n",
    "\n",
    "N_components = 100    #<-- number of Gaussian components\n",
    "cov_type = 'spherical'   #<-- choices are:  ‘full’ , ‘tied’ , ‘diag’ , ‘spherical’\n",
    "init_params ='random'#<-- every time begins at a different point\n",
    "max_iter=1000       #<-- number of iterations before stop (if not convergence)\n",
    "\n",
    "\n",
    "gmm_0 = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "gmm_1 = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "\n",
    "\n",
    "gmm_0.fit(X0_train,Y0_train)\n",
    "gmm_1.fit(X1_train,Y1_train)\n",
    "\n",
    "\n",
    "# LANZAMOS LOS TESTS Y CALCULAMOS LAS ETIQUETAS (EL GMM QUE DA EL SCORE MAXIMO PARA CADA EJEMPLO)\n",
    "\n",
    "\n",
    "n0_scores = np.exp(gmm_0.score_samples(X_test))\n",
    "n1_scores = np.exp(gmm_1.score_samples(X_test))\n",
    "\n",
    "\n",
    "Y_hat = []\n",
    "for i in range(0,len(n1_scores)):\n",
    "    tag = np.argmax([n0_scores[i],n1_scores[i]])\n",
    "    if tag == 0:\n",
    "        tag = 0\n",
    "    else:\n",
    "        tag = 1\n",
    "    Y_hat = np.append(Y_hat,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"> \n",
    " <tr style='background:pink'>\n",
    "  <td style=\"text-align:left\">\n",
    "      <h2>A partir de aquí ya no se pueden modificar las celdas</h2>\n",
    "          <h3>Comprueba que:</h3>\n",
    "          <p> $\\quad \\bullet$ tu clasificador está almacenado en la variable <b>clf</b><br>\n",
    "              $\\quad \\bullet$ tienes el conjunto de test correctamente almacenado en la variable <b>X_test</b><br>\n",
    "              $\\quad \\bullet$ tienes las etiquetas del conjunto de test correctamente almacenadas en la variable <b>Y_test</b><br>\n",
    "          </p>\n",
    "      \n",
    "  </td>\n",
    " </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adrian Lopez'] \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[358  88]\n",
      " [ 20 485]] \n",
      "\n",
      "Outcome:\n",
      "\n",
      "  :) HIT  = 843, (88.64%)\n",
      "  :( FAIL = 108, (11.36%)\n"
     ]
    }
   ],
   "source": [
    "# NO TOCAR ESTA CELDA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y_hat = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(Y_test , Y_hat)\n",
    "N_success  = np.trace(conf_mat)\n",
    "N_fails = Y_test.shape[0]-N_success\n",
    "#-------------------------------\n",
    "print (nombres,\"\\n\")\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(conf_mat,\"\\n\")\n",
    "print(\"Outcome:\\n\")\n",
    "strlog = \"  :) HIT  = %d, (%0.2f%%)\"%(N_success, 100*N_success/(N_success+N_fails))\n",
    "print(strlog)\n",
    "strlog = \"  :( FAIL = %d, (%0.2f%%)\"%(N_fails, 100*N_fails/(N_success+N_fails))\n",
    "print(strlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
