{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 2: Problema multiclase\n",
    "\n",
    "Este reto consiste en aprender a clasificar 4 tipos diferentes de vehículos utilizando cualquiera de los clasificadores o técnicas estudiadas hasta el momento. Esto incluye:\n",
    "+ clasificación lineal\n",
    "+ transformaciones no lineales seguido de un clasificador lineal\n",
    "+ Support Vector Machines (SVM)\n",
    "+ Decision Tree (DT)\n",
    "\n",
    "Además se pueden aplicar técnicas de preprocesado como:\n",
    "+ escalado de las características\n",
    "+ *grid search* para búsqueda de hiperparámetros\n",
    "+ validación cruzada\n",
    "\n",
    "El conjunto de datos, *vehiculos_reto2.csv*, consiste en 592 muestras de vehículos; cada uno de ellos representado por 18 características.\n",
    "\n",
    "Para evaluar las propuestas se utilizará un conjunto de datos que se mantendrá oculto hasta después de la entrega\n",
    "\n",
    "### Requisitos\n",
    "+ La entrega se realiza **sólo** a través de la tarea habilitada para ello en la pestaña de *Evaluación* del Aula Virtual.\n",
    "+ Se debe entregar un cuaderno Jupyter con el nombre de los participantes.<br>\n",
    "  *Por ejemplo*:   **Cuesta_LeCunn.ipynb**\n",
    "+ El cuaderno entregado debe seguir la estructura y reglas de este cuaderno\n",
    "\n",
    "### Competición\n",
    "+ Todos los cuadernos entregados se subirán al repo de GitHub y se ejecutarán en Binder, donde ya estará en conjunto de test que permanecía oculto.\n",
    "+ El número de aciertos respecto del número de ejemplos será la puntuación del reto.\n",
    "+ **Importante** Es muy fácil asegurarte de que tu código funcionará bien. Para ello:\n",
    "    1. Agrupa todo tu código en una única celda\n",
    "    2. En el cuaderno del reto que hay en Binder: elimina las celdas que hay entre la verde y la roja, y copia tu celda entre ellas.\n",
    "    3. Ejecuta ese cuaderno de Binder. \n",
    "    \n",
    "### Plazo: lunes 26 de oct. de 2020 a las 6 am.\n",
    "Es decir, incluye toda la noche del domingo 25 de oct.\n",
    "\n",
    "\n",
    "---\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto distribuido para el reto\n",
    "\n",
    "Challange_filename = '../../Datasets/vehiculos_reto2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto NO distribuido para evaluar los clasificadores entregados\n",
    "\n",
    "Test_filename = '../../Datasets/vehiculos_test.csv' #<-- este nombre cambiará después del plazo de entrega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-[1]. Load data from CSV and put all in a single dataframe 'FullSet'\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../../MyUtils/')\n",
    "import MyUtils as my\n",
    "seed = 1234 #<- random generator seed (comment to get randomness)\n",
    "\n",
    "#-[2]. Load data from CSV and put all in a single dataframe 'FullSet'\n",
    "\n",
    "FullSet = pd.read_csv(Challange_filename, header=0)\n",
    "FullX = FullSet.drop('Class', axis=1)\n",
    "FullY = FullSet[['Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"> \n",
    " <tr style='background:lime'>\n",
    "  <td style=\"text-align:left\">\n",
    "      <h2>Tu código debe empezar a partir de aquí y puede tener tantas celdas como quieras</h2>\n",
    "      <p> Si quieres, puedes borrar (o convertir en RawNBConvert) las celdas de ejemplo\n",
    "      <h3>Importante:</h3>\n",
    "      <p>Tu código debe producir las siguientes variables: </p>\n",
    "      <p> $\\quad \\bullet$ <b>clf:</b> el clasificador final con el que se realizará el test<br>\n",
    "       $\\quad \\bullet$ <b>X_test:</b> el conjunto de test listo para ser usado por el método <b>predict</b><br>\n",
    "       $\\quad \\bullet$ <b>Y_test:</b> es el vector de etiquetas del conjunto de X_test listo para ser usado por el método <b>confusion_matrix</b>\n",
    "      </p>\n",
    "  </td>\n",
    " </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = [\"Adrian Lopez\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PREPARACION DE LOS DATOS #############################\n",
    "\n",
    "valid_size = 0.25\n",
    "X_train, Y_train, X_test, Y_test = \\\n",
    "   my.single_stratified_split(FullX,FullY, test_size=valid_size, random_state=seed)\n",
    "\n",
    "\n",
    "#Para el algoritmo genetico se ha usado un conjunto adicional de validación\n",
    "# X_train, Y_train, X_valid, Y_valid = \\\n",
    "#     my.single_stratified_split(FullX,FullY, test_size=valid_size, random_state=seed)\n",
    "\n",
    "\n",
    "# Puesto que las SVM son muy dependientes del tamaño de entrenamiento y tenemos un DATASET limitado se ha decidido usar\n",
    "# todo el DATASET para entrenamiento y un subconjunto de este para test\n",
    "X_train = FullX\n",
    "Y_train = FullY\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler() #LA ESTANDARIZACION DA MEJORES RESULTADOS QUE LA NORMALIZACION\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "Y_train = Y_train.values.ravel() \n",
    "\n",
    "# Preparación del test para la realización del reto\n",
    "# X_test = scaler.transform(X_test)\n",
    "# Y_test = Y_test.values.ravel() \n",
    "\n",
    "\n",
    "# Preparación del test para la correción del reto\n",
    "FullSet = pd.read_csv(Test_filename, header=0)\n",
    "TestX_ = FullSet.drop('Class', axis=1)\n",
    "TestY_ = FullSet[['Class']]\n",
    "X_test = scaler.transform(TestX_)\n",
    "Y_test = TestY_.values.ravel() "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PRUEBA CON DT: TASAS DE ACIERTO DE 75% APROX: MUCHO PEOR QUE SVMS\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth = 20\n",
    "clf = DecisionTreeClassifier(random_state=seed)\n",
    "clf.fit(X_train,Y_train) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# SVM: PRUEBA CON ALGORITMO GENÉTICO (ES UNA ADAPTACIÓN DE UN CÓDIGO QUE DESARROLLE HACE TIEMPO PARA ENTRENAMIENTO NO SUPERVISADO DE NNs)\n",
    "\n",
    "# FUNCIONA MAS O MENOS (HAY QUE JUGAR MUCHO CON LOS PARAMÉTROS DE MUTATION) PERO REQUIERE DE TIEMPOS MUY ELEVADOS PARA LOGRAR RESULTADOS SIMILARES A LOS DEL GRIDSEARCH POR LO QUE SE HA DESCARTADO SU USO\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random as rand\n",
    "\n",
    "\n",
    "N = 100\n",
    "C = np.ones(N)\n",
    "gamma = np.ones(N)\n",
    "MUTATION_RATE = 0.9\n",
    "MUTATION_SIZE = 1\n",
    "\n",
    "def mutation(C,gamma):\n",
    "    mutations_C = np.random.random_sample(size = N)\n",
    "    mutations_gamma = np.random.random_sample(size = N)\n",
    "    for n in range(0,N):\n",
    "        if (mutations_C[n] <= MUTATION_RATE):\n",
    "            random_value = np.random.uniform(-MUTATION_SIZE, MUTATION_SIZE)\n",
    "            C[n] = C[n] * (1 + random_value)\n",
    "        if (mutations_gamma[n] <= MUTATION_RATE):\n",
    "            random_value = np.random.uniform(-MUTATION_SIZE, MUTATION_SIZE)\n",
    "            gamma[n] = gamma[n] * (1 + random_value)\n",
    "    return C, gamma\n",
    "\n",
    "for epochs in range(0,1000):\n",
    "    C, gamma = mutation(C,gamma)\n",
    "    scores = []\n",
    "    for n in range(0,N):\n",
    "        clf = SVC(random_state = seed, C=C[n], gamma = gamma[n])\n",
    "        clf.fit(X_train,Y_train)\n",
    "        scores = np.append(scores,clf.score(X_valid,Y_valid))\n",
    "    best = np.argmax(scores)\n",
    "    print(scores[best])\n",
    "    C = np.full((100),C[best])\n",
    "    gamma = np.full((100),gamma[best])\n",
    "    \n",
    "clf = SVC(random_state = seed, C=C[best], gamma = gamma[best])\n",
    "clf.fit(np.append(X_train,X_valid),np.append(Y_train,Y_valid))          "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SVM: PRUEBA DE GRIDSEARCH CON VALIDACION CRUZADA\n",
    "\n",
    "# SE HA DESCARTADO POR LA LIMITACION DEL TAMAÑO DEL DATASET\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random as rand\n",
    "\n",
    "#PARA PROBAR GRANDES CONJUNTOS DE HIPERPARÁMETROS ESTOS SE GENERAN USANDO SERIES GEOMÉTRICAS\n",
    "\n",
    "C = []\n",
    "gamma = []\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    C = np.append(C,aux)\n",
    "    aux = aux*1.07\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    gamma = np.append(gamma,aux)\n",
    "    aux = aux*1.07\n",
    "    \n",
    "    \n",
    "# C = [0.0001,0.001,0.01,0.1,10,100,1000,10000]\n",
    "# gamma = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "params = [{'kernel': ['rbf'],'C' : C, 'gamma' : gamma}]\n",
    "clf = GridSearchCV(SVC(random_state = seed,decision_function_shape = 'ovo'),params,cv=4)\n",
    "# clf = SVC(kernel='rbf', C=156.84880557968373, gamma=0.47201372444611994, random_state = seed, decision_function_shape = 'ovr')\n",
    "clf.fit( X_train, Y_train )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# SVM: PRUEBA DE GRIDSEARCH SIN VALIDACION CRUZADA\n",
    "\n",
    "# -------------- IMPORTANTE: ESTE ES EL PROCESO QUE MEJORES RESULTADOS HA DADO ---------------------\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# best C =     156.84880557968373  (ovr)         14746.542149527892   (ovo)\n",
    "# best gamma = 0.47201372444611994 (ovr)         0.004420705158732798 (ovo)\n",
    "\n",
    "scores = []\n",
    "clfs = []\n",
    "\n",
    "C = []\n",
    "gamma = []\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    C = np.append(C,aux)\n",
    "    aux = aux*1.07\n",
    "aux = 0.0001\n",
    "for i in range(1,300):\n",
    "    gamma = np.append(gamma,aux)\n",
    "    aux = aux*1.07\n",
    "\n",
    "contador = 0\n",
    "for i in C:\n",
    "    contador+=1\n",
    "    for j in gamma:\n",
    "        clf = SVC(kernel='rbf', C=i, gamma=j, random_state = seed, decision_function_shape = 'ovo')\n",
    "        clf.fit(X_train,Y_train)\n",
    "        scores = np.append(scores,clf.score(X_valid,Y_valid))\n",
    "        clfs = np.append(clfs,clf)\n",
    "    print(contador/3,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=14746.542149527892, decision_function_shape='ovo',\n",
       "    gamma=0.004420705158732798, random_state=1234)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREAMOS LA SVM CON LOS HIPERPARÁMETROS SELECCIONADOS USANDO LOS MÉTODO ANTERIORES\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', C=14746.542149527892, gamma=0.004420705158732798, random_state = seed, decision_function_shape = 'ovo')\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# GMM: DA BUENOS RESULTADOS PERO NO ES CAPAZ DE SEPARAR LAS CLASES OPEL DE VAN (APROX. 97 ACIERTO EN EL RESTO)\n",
    "\n",
    "# PREPARACIÓN DE DATOS\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_size = 0.25\n",
    "\n",
    "# Dividimos el DataSet por etiquetas y escalamos las características de cada una por separado\n",
    "Full_saab = FullSet.drop(FullSet[FullSet[\"Class\"]!=\"saab\"].index).reset_index()\n",
    "Full_bus = FullSet.drop(FullSet[FullSet[\"Class\"]!=\"bus\"].index).reset_index()\n",
    "Full_van = FullSet.drop(FullSet[FullSet[\"Class\"]!=\"van\"].index).reset_index()\n",
    "Full_opel = FullSet.drop(FullSet[FullSet[\"Class\"]!=\"opel\"].index).reset_index()\n",
    "\n",
    "Full_saabX = Full_saab.drop('Class', axis=1)\n",
    "Full_saabY = Full_saab[['Class']]\n",
    "\n",
    "Full_busX = Full_bus.drop('Class', axis=1)\n",
    "Full_busY = Full_bus[['Class']]\n",
    "\n",
    "Full_vanX = Full_van.drop('Class', axis=1)\n",
    "Full_vanY = Full_van[['Class']]\n",
    "\n",
    "Full_opelX = Full_opel.drop('Class', axis=1)\n",
    "Full_opelY = Full_opel[['Class']]\n",
    "\n",
    "XSaab_train, YSaab_train, XSaab_test, YSaab_test = \\\n",
    "   my.single_stratified_split(Full_saabX,Full_saabY, test_size=test_size, random_state=seed)\n",
    "XBus_train, YBus_train, XBus_test, YBus_test = \\\n",
    "   my.single_stratified_split(Full_busX,Full_busY, test_size=test_size, random_state=seed)\n",
    "XVan_train, YVan_train, XVan_test, YVan_test = \\\n",
    "   my.single_stratified_split(Full_vanX,Full_vanY, test_size=test_size, random_state=seed)\n",
    "XOpel_train, YOpel_train, XOpel_test, YOpel_test = \\\n",
    "   my.single_stratified_split(Full_opelX,Full_opelY, test_size=test_size, random_state=seed)\n",
    "\n",
    "\n",
    "# scalerSaab = StandardScaler()\n",
    "# scalerBus = StandardScaler()\n",
    "# scalerVan = StandardScaler()\n",
    "# scalerOpel = StandardScaler()\n",
    "\n",
    "scalerSaab = MinMaxScaler()\n",
    "scalerBus = MinMaxScaler()\n",
    "scalerVan = MinMaxScaler()\n",
    "scalerOpel = MinMaxScaler()\n",
    "\n",
    "XSaab_train = scalerSaab.fit_transform(XSaab_train)\n",
    "YSaab_train = YSaab_train.values.ravel() \n",
    "\n",
    "XBus_train = scalerBus.fit_transform(XBus_train)\n",
    "YBus_train = YBus_train.values.ravel()\n",
    "\n",
    "XVan_train = scalerVan.fit_transform(XVan_train)\n",
    "YVan_train = YVan_train.values.ravel()\n",
    "\n",
    "XOpel_train = scalerVan.fit_transform(XOpel_train)\n",
    "YOpel_train = YOpel_train.values.ravel()\n",
    "\n",
    "\n",
    "XSaab_test = scalerSaab.transform(XSaab_test)\n",
    "YSaab_test = YSaab_test.values.ravel() \n",
    "\n",
    "XBus_test = scalerBus.transform(XBus_test)\n",
    "YBus_test = YBus_test.values.ravel()\n",
    "\n",
    "XVan_test = scalerVan.transform(XVan_test)\n",
    "YVan_test = YVan_test.values.ravel()\n",
    "\n",
    "XOpel_test = scalerVan.transform(XOpel_test)\n",
    "YOpel_test = YOpel_test.values.ravel()\n",
    "\n",
    "# X_test = [XSaab_test,XBus_test,XVan_test,XOpel_test]\n",
    "X_test = np.append(XSaab_test,np.append(XBus_test,np.append(XVan_test,XOpel_test,axis=0),axis=0),axis=0)\n",
    "Y_test = np.append(YSaab_test,np.append(YBus_test,np.append(YVan_test,YOpel_test,axis=0),axis=0),axis=0)\n",
    "\n",
    "\n",
    "# CREAMOS LOS GMM (1 PARA CADA CLASE)\n",
    "\n",
    "N_components = 5    #<-- number of Gaussian components\n",
    "cov_type = 'tied'   #<-- choices are:  ‘full’ , ‘tied’ , ‘diag’ , ‘spherical’\n",
    "init_params='random'#<-- every time begins at a different point\n",
    "max_iter=1000       #<-- number of iterations before stop (if not convergence)\n",
    "\n",
    "\n",
    "gmm_saab = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "gmm_bus = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "gmm_van = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "gmm_opel = GaussianMixture(n_components=N_components, \\\n",
    "                        covariance_type=cov_type, init_params=init_params, max_iter=max_iter)\n",
    "\n",
    "gmm_saab.fit(XSaab_train,YSaab_train)\n",
    "gmm_bus.fit(XBus_train,YBus_train)\n",
    "gmm_van.fit(XVan_train,YVan_train)\n",
    "gmm_opel.fit(XOpel_train,YOpel_train)\n",
    "\n",
    "\n",
    "# LANZAMOS LOS TESTS Y CALCULAMOS LAS ETIQUETAS (EL GMM QUE DA EL SCORE MAXIMO PARA CADA EJEMPLO)\n",
    "\n",
    "saab_scores = np.exp(gmm_saab.score_samples(X_test))\n",
    "bus_scores = np.exp(gmm_bus.score_samples(X_test))\n",
    "van_scores = np.exp(gmm_van.score_samples(X_test))\n",
    "opel_scores = np.exp(gmm_opel.score_samples(X_test))\n",
    "\n",
    "Y_hat = []\n",
    "for i in range(0,len(saab_scores)):\n",
    "    tag = np.argmax([saab_scores[i],bus_scores[i],van_scores[i],opel_scores[i]])\n",
    "    if tag == 0:\n",
    "        tag = 'saab'\n",
    "    elif tag == 1 :\n",
    "        tag = 'bus'\n",
    "    elif tag == 2:\n",
    "        tag = 'van'\n",
    "    else:\n",
    "        tag = 'opel'\n",
    "    Y_hat = np.append(Y_hat,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%;\"> \n",
    " <tr style='background:pink'>\n",
    "  <td style=\"text-align:left\">\n",
    "      <h2>A partir de aquí ya no se pueden modificar las celdas</h2>\n",
    "          <h3>Comprueba que:</h3>\n",
    "          <p> $\\quad \\bullet$ tu clasificador está almacenado en la variable <b>clf</b><br>\n",
    "              $\\quad \\bullet$ tienes el conjunto de test correctamente almacenado en la variable <b>X_test</b><br>\n",
    "              $\\quad \\bullet$ tienes las etiquetas del conjunto de test correctamente almacenadas en la variable <b>Y_test</b><br>\n",
    "          </p>\n",
    "      \n",
    "  </td>\n",
    " </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adrian Lopez'] \n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[63  1  0  1]\n",
      " [ 0 46 16  2]\n",
      " [ 2 15 48  0]\n",
      " [ 0  2  1 57]] \n",
      "\n",
      "Outcome:\n",
      "\n",
      "  :) HIT  = 214, (84.25%)\n",
      "  :( FAIL = 40, (15.75%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_hat = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(Y_test , Y_hat)\n",
    "N_success  = np.trace(conf_mat)\n",
    "N_fails = Y_test.shape[0]-N_success\n",
    "#-------------------------------\n",
    "print (nombres,\"\\n\")\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(conf_mat,\"\\n\")\n",
    "print(\"Outcome:\\n\")\n",
    "strlog = \"  :) HIT  = %d, (%0.2f%%)\"%(N_success, 100*N_success/(N_success+N_fails))\n",
    "print(strlog)\n",
    "strlog = \"  :( FAIL = %d, (%0.2f%%)\"%(N_fails, 100*N_fails/(N_success+N_fails))\n",
    "print(strlog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
