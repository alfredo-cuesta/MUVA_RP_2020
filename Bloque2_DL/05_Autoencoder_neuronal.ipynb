{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KI5HO6c0MQy"
   },
   "source": [
    "# Autoencoder neuronal\n",
    "\n",
    "En este cuaderno se construye un autoencoder formado por capas densas y se prueba con el MNIST\n",
    "\n",
    "Es importante destacar que el problema que se aborda con el autoencoder es el de la reconstrucción de la imagen<br>\n",
    "Por tanto se trata de una tarea **NO supervisada** ya que las imágenes que se utilizan no están etiquetadas. \n",
    "\n",
    "---\n",
    "\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminares\n",
    "Paquetes de propósito general (_numpy_, _matplotlib_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kd8kmCJ8FzY"
   },
   "outputs": [],
   "source": [
    "#-[0]. General purpose packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a36yDHQK0Tny"
   },
   "source": [
    "**Cargar el MNIST**\n",
    "\n",
    "Observa que no es necesario cargar las etiquetas puesto que NO las vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3090,
     "status": "ok",
     "timestamp": 1564080372050,
     "user": {
      "displayName": "alfredo cuesta infante Universidad Rey Juan Carlos",
      "photoUrl": "https://lh4.googleusercontent.com/-z9Tr7G7VUMk/AAAAAAAAAAI/AAAAAAAAAAc/gH3qm0UbcIo/s64/photo.jpg",
      "userId": "17488335604138000921"
     },
     "user_tz": -120
    },
    "id": "1-qDDIYSvF0W",
    "outputId": "39b5aa89-d756-403c-e01f-c78416137732"
   },
   "outputs": [],
   "source": [
    "#-[1]. Load images. Keras has a few benchmark datasets readily available.\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "#--- Get info of train and test data sets\n",
    "N_train,dim0,dim1 = x_train.shape\n",
    "N_test,dim0,dim1  = x_test.shape\n",
    "num_pixels = dim0*dim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjWNP6d00bdG"
   },
   "source": [
    "### Construcción del autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_stack4(x, neuron_list, activation=\"relu\"):\n",
    "    h1 = Dense(neuron_list[0], activation=activation)(x)\n",
    "    h2 = Dense(neuron_list[1], activation=activation)(h1)\n",
    "    h3 = Dense(neuron_list[2], activation=activation)(h2)\n",
    "    y  = Dense(neuron_list[3], activation=activation)(h3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3106,
     "status": "ok",
     "timestamp": 1564080372080,
     "user": {
      "displayName": "alfredo cuesta infante Universidad Rey Juan Carlos",
      "photoUrl": "https://lh4.googleusercontent.com/-z9Tr7G7VUMk/AAAAAAAAAAI/AAAAAAAAAAc/gH3qm0UbcIo/s64/photo.jpg",
      "userId": "17488335604138000921"
     },
     "user_tz": -120
    },
    "id": "RtdiOOonwUcU",
    "outputId": "78cc47cd-f906-48aa-9ca3-2565add89761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               30300     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               235984    \n",
      "=================================================================\n",
      "Total params: 544,104\n",
      "Trainable params: 544,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#-[2]. Modeling the neural network in three different ways\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "neuron_list = [num_pixels, 300, 100, 50, 20]\n",
    "\n",
    "input_model = Input( shape=(num_pixels) )\n",
    "\n",
    "code = dense_stack4( input_model, neuron_list[1:] )\n",
    "\n",
    "output_model = dense_stack4( code, neuron_list[-2::-1])\n",
    "\n",
    "encoder = Model(input_model,code)\n",
    "autoencoder = Model(input_model,output_model)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1r2xGCF1tkW"
   },
   "source": [
    "### _Compilar_ el modelo\n",
    "\n",
    "+ Para compilar el modelo tenemos que seleccionar un optimizador y una función de pérdida.\n",
    "+ También se puede elegir un conjunto de métricas para evaluar el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CfLnCDn7trc"
   },
   "source": [
    "## Aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWpsv3RW4c1u"
   },
   "source": [
    "**1. Preparamos el conjunto de datos para que pueda ser procesado por el modelo**  <br>\n",
    "       _En este caso tenemos que serializar las imágenes, de matrices $28\\times28$ a vectores de $784$_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYqb-Py14xRK"
   },
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape( (N_train,num_pixels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjTuHuom21ng"
   },
   "source": [
    "**2. Ejecutar el método FIT**<br>\n",
    "\n",
    "Aunque el problema sea NO supervisado, las redes neuronales necesitan un valor objetivo o *target* al que las salidas deben aproximarse, y con el que calcular la pérdida. \n",
    "\n",
    "En el caso de los autoencoders, es la misma imagen de entrada. <br>\n",
    "Por eso el método fit recibe `x_train_flat` tanto en el parámetro *x* como *y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31742,
     "status": "ok",
     "timestamp": 1564080400818,
     "user": {
      "displayName": "alfredo cuesta infante Universidad Rey Juan Carlos",
      "photoUrl": "https://lh4.googleusercontent.com/-z9Tr7G7VUMk/AAAAAAAAAAI/AAAAAAAAAAc/gH3qm0UbcIo/s64/photo.jpg",
      "userId": "17488335604138000921"
     },
     "user_tz": -120
    },
    "id": "Ta4SlQ_x244O",
    "outputId": "4d6df649-cdc1-448d-de82-30b2c3b16233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 2354.1602\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 1758.3898\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 1658.0315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7460553d10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "N_epochs = 3\n",
    "batch_size = 32\n",
    "autoencoder.fit(x_train_flat, x_train_flat, epochs=N_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCOOvn-U8yIy"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFV393Fb9SS2"
   },
   "source": [
    "**1. Debemos procesar los datos de test igual que procesamos los de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_flat = x_test.reshape( (N_test,num_pixels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ejecutar el método PREDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed = autoencoder.predict( x_test_flat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7460553bd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 123\n",
    "plt.subplot(121)\n",
    "plt.imshow(x_test[k,:].reshape([28,28]))\n",
    "plt.subplot(122)\n",
    "plt.imshow(x_reconstructed[k,:].reshape([28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "a36yDHQK0Tny",
    "kjWNP6d00bdG",
    "3CfLnCDn7trc",
    "JCOOvn-U8yIy"
   ],
   "name": "01_keras_FCNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
