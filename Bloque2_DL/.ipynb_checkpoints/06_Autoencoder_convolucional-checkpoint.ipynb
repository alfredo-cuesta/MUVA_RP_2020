{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KI5HO6c0MQy"
   },
   "source": [
    "# Autoencoders convolucionales \n",
    "\n",
    "En este cuaderno vamos a construir autoencoders convolucionales.\n",
    "\n",
    "A diferencia de los autoencoders neuronales, aquí necesitamos una operación que *deshaga* el MaxPooling2D.\n",
    "\n",
    "---\n",
    "\n",
    "    [ES] Código de Alfredo Cuesta Infante para 'Reconocimiento de Patrones'\n",
    "       @ Master Universitario en Visión Artificial, 2020, URJC (España)\n",
    "    [EN] Code by Alfredo Cuesta-Infante for 'Pattern Recognition'\n",
    "       @ Master of Computer Vision, 2020, URJC (Spain)\n",
    "\n",
    "    alfredo.cuesta@urjc.es    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kd8kmCJ8FzY"
   },
   "outputs": [],
   "source": [
    "#-[0]. General purpose packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a36yDHQK0Tny"
   },
   "source": [
    "**Cargar el MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8585,
     "status": "ok",
     "timestamp": 1573748153656,
     "user": {
      "displayName": "alfredo cuesta infante Universidad Rey Juan Carlos",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD0b4XSkrh_rJCoTmgJ-8PhQuMJ0ghZkDfkaDYb=s64",
      "userId": "17488335604138000921"
     },
     "user_tz": -60
    },
    "id": "1-qDDIYSvF0W",
    "outputId": "1572443e-5264-4a48-9c42-d0b9957728b1"
   },
   "outputs": [],
   "source": [
    "#-[1]. Load images. Keras has a few benchmark datasets readily available.\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "#--- Get info of train and test data sets\n",
    "N_train,dim0,dim1 = x_train.shape\n",
    "N_test,dim0,dim1  = x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjWNP6d00bdG"
   },
   "source": [
    "### Construcción de la red\n",
    "En esta celda hay varias cosas interesantes:\n",
    "+ Como las imágenes del MNIST son $28\\times28$, cuando aplicamos `MaxPooling2D((2,2))` 2 veces obtenemos un mapa de características $7\\times7$. <br>\n",
    "Si volvemos a aplicar `MaxPooling2D((2,2))`, como ambas dimensiones son impares, y el padding por defecto es \"same\", el resultado $3\\times3$. <br>\n",
    "Pero esto supone un problema al reconstruir el código latente, porque acabamos en una imágen que no tiene las mismas dimensiones que la imagen de entrada.\n",
    "+ Este problema se ha resuelto modificando el padding como se muestra en el comentario del código.\n",
    "+ Otro tema interesante es que se ha utilizad una activación lineal para la última capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 5)         50        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 15)        690       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 15)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 25)          3400      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 25)          5650      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 25)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 25)          5650      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 25)        5650      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 1)         226       \n",
      "=================================================================\n",
      "Total params: 21,316\n",
      "Trainable params: 21,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def encoder3 (x, filter_list, activation=\"relu\"):\n",
    "    h1 = Conv2D(filter_list[0], (3,3), activation=activation, padding=\"same\")(x)\n",
    "    p1 = MaxPooling2D((2,2))(h1)\n",
    "    h2 = Conv2D(filter_list[1], (3,3), activation=activation, padding=\"same\")(p1)\n",
    "    p2 = MaxPooling2D((2,2))(h2)\n",
    "    h3 = Conv2D(filter_list[2], (3,3), activation=activation, padding=\"same\")(p2)\n",
    "    code = MaxPooling2D((2,2), padding=\"same\")(h3) #<-- padding = \"same\" !!\n",
    "    return code\n",
    "    \n",
    "def decoder3 (x, filter_list, activation=\"relu\"):\n",
    "    h1 = Conv2D(filter_list[0], (3, 3), activation='relu', padding='same')(x)\n",
    "    h2 = UpSampling2D((2, 2))(h1)\n",
    "    h3 = Conv2D(filter_list[1], (3, 3), activation='relu', padding='same')(h2)\n",
    "    h4 = UpSampling2D((2, 2))(h3)\n",
    "    h5 = Conv2D(filter_list[2], (3, 3), activation='relu', padding='valid')(h4) #<-- padding = \"valid\" !!\n",
    "    h6 = UpSampling2D((2, 2))(h5)    \n",
    "    y  = Conv2D(1,(3,3),  activation='linear', padding='same')(h6) \n",
    "    return y\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.layers import Flatten, Dense, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#- Input layer\n",
    "x = Input( shape=(dim0, dim1, 1) ) \n",
    "#- encoder with 3 conv.layers\n",
    "filter_list = [5,15,25]\n",
    "code = encoder3(x, filter_list)\n",
    "#- decoder with 3 conv.layers\n",
    "filter_list = [25,15,5]\n",
    "x_pred = decoder3(code, filter_list)\n",
    "#- Put all in a model and compile\n",
    "ae = Model(x,x_pred)\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CfLnCDn7trc"
   },
   "source": [
    "## Aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Preparamos el conjunto de datos para que pueda ser procesado por el modelo**  <br>\n",
    "       _En este caso NO hace falta serializar las imágenes ya que la entrada de las CNN es típicamente imágenes._<br>\n",
    "       Es importante reflejar la profundidad de color. Normalmente será 1 para imágenes en escala de grises y 3 para RGB. <br>\n",
    "       Pero podría ser otro, por ejemplo 4 para RGB-D\n",
    "       \n",
    "**2. Preparamos también el vector de etiquetas para que tenga una representación 1-hot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYqb-Py14xRK"
   },
   "outputs": [],
   "source": [
    "x_tensor = x_train.reshape((N_train,dim0,dim1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Ejecutar el método FIT**<br>\n",
    "Tensorflow sigue el estandar de Scikit-Learn. Pero a diferencia de otros métodos que hemos visto de ML, en DL hay que especificar algunas otra opciones como el número de épocas o el tamaño del lote.\n",
    "+ El **número de épocas** indica cuantas veces se utiliza el conjunto de entrenamiento para realizar el aprendizaje\n",
    "+ El **tamaño del lote** es el número de muestras que se utilizan para calcular el descenso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 502206,
     "status": "ok",
     "timestamp": 1573749611159,
     "user": {
      "displayName": "alfredo cuesta infante Universidad Rey Juan Carlos",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mD0b4XSkrh_rJCoTmgJ-8PhQuMJ0ghZkDfkaDYb=s64",
      "userId": "17488335604138000921"
     },
     "user_tz": -60
    },
    "id": "Ta4SlQ_x244O",
    "outputId": "10b115c4-36a5-44da-b695-ec45e7d80764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 93s 50ms/step - loss: 1187.8959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3bb407c550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "\n",
    "N_epochs = 1\n",
    "batch_size = 32\n",
    "ae.fit(x_tensor, x_tensor, epochs=N_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Debemos procesar los datos de test igual que procesamos los de entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_flat = x_test.reshape((N_test,dim0,dim1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ejecutar el método PREDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed = ae.predict( x_test_flat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3bac0ade50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 123\n",
    "plt.subplot(121)\n",
    "plt.imshow(x_test[k,:].reshape([28,28]))\n",
    "plt.subplot(122)\n",
    "plt.imshow(x_reconstructed[k,:].reshape([28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "a36yDHQK0Tny",
    "kjWNP6d00bdG",
    "4w2Y9htI2Ix2"
   ],
   "name": "02_keras_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
